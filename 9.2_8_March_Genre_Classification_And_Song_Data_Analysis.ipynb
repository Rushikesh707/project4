{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f51599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ajinkya\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\ajinkya\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "# Installing pyspark on local machine\n",
    "!pip install pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ca9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing installed pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6816fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PySpark Session \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b12dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark and setting a name for the application\n",
    "spark=SparkSession.builder.appName('Project').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef46aa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://SeedingYounger:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eeb71d6790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc9bea",
   "metadata": {},
   "source": [
    "REMOVING STRUCTURAL ERRORS FROM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c31e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv file using spark\n",
    "music = spark.read.csv(r\"C:\\Users\\AJINKYA\\Desktop\\Final_Project-main\\final_spotify.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee5de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+----+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "|     _c0|                 _c1|                 _c2|                 _c3|                 _c4|       _c5|        _c6|     _c7|         _c8|   _c9|_c10|    _c11|_c12|       _c13|        _c14|            _c15|    _c16|   _c17|   _c18|          _c19|       _c20|                _c21|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+----+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "|index_id|            track_id|             artists|          album_name|          track_name|popularity|duration_ms|explicit|danceability|energy| key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|time_signature|track_genre|spotify_release_date|\n",
      "|       0|5SuOikwiRyPMVoIQD...|         Gen Hoshino|              Comedy|              Comedy|        73|     230666|   FALSE|       0.676| 0.461|   1|  -6.746|   0|      0.143|      0.0322|        1.01E-06|   0.358|  0.715| 87.917|             4|   acoustic|          2016-12-31|\n",
      "|       1|4qPNDBW1i3p13qLCt...|        Ben Woodward|    Ghost (Acoustic)|    Ghost - Acoustic|        55|     149610|   FALSE|        0.42| 0.166|   1| -17.235|   1|     0.0763|       0.924|        5.56E-06|   0.101|  0.267| 77.489|             4|   acoustic|          2009-02-23|\n",
      "|       2|1iJBSr7s7jYXzM8EG...|Ingrid Michaelson...|      To Begin Again|      To Begin Again|        57|     210826|   FALSE|       0.438| 0.359|   0|  -9.734|   1|     0.0557|        0.21|               0|   0.117|   0.12| 76.332|             4|   acoustic|          2014-03-26|\n",
      "|       3|6lfxq3CG4xtTiEg7o...|        Kina Grannis|Crazy Rich Asians...|Can't Help Fallin...|        71|     201933|   FALSE|       0.266|0.0596|   0| -18.515|   1|     0.0363|       0.905|        7.07E-05|   0.132|  0.143| 181.74|             3|   acoustic|          2013-02-01|\n",
      "|       4|5vjLSffimiIP26QG5...|    Chord Overstreet|             Hold On|             Hold On|        82|     198853|   FALSE|       0.618| 0.443|   2|  -9.681|   1|     0.0526|       0.469|               0|  0.0829|  0.167|119.949|             4|   acoustic|          2011-12-14|\n",
      "|       5|01MVOl9KtVTNfFiBU...|        Tyrone Wells|Days I Will Remember|Days I Will Remember|        58|     214240|   FALSE|       0.688| 0.481|   6|  -8.807|   1|      0.105|       0.289|               0|   0.189|  0.666| 98.017|             4|   acoustic|          2013-08-31|\n",
      "|       6|6Vc5wAMmXdKIAM7WU...|A Great Big World...|Is There Anybody ...|       Say Something|        74|     229400|   FALSE|       0.407| 0.147|   2|  -8.822|   1|     0.0355|       0.857|        2.89E-06|  0.0913| 0.0765|141.284|             3|   acoustic|          2020-08-23|\n",
      "|       7|1EzrEOXmMH3G43AXT...|          Jason Mraz|We Sing. We Dance...|           I'm Yours|        80|     242946|   FALSE|       0.703| 0.444|  11|  -9.331|   1|     0.0417|       0.559|               0|  0.0973|  0.712| 150.96|             4|   acoustic|          2019-08-29|\n",
      "|       8|0IktbUcnAGrvD03AW...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        74|     189613|   FALSE|       0.625| 0.414|   0|    -8.7|   1|     0.0369|       0.294|               0|   0.151|  0.669|130.088|             4|   acoustic|          2018-03-18|\n",
      "|       9|7k9GuJYLp2AzqokyE...|      Ross Copperman|              Hunger|              Hunger|        56|     205594|   FALSE|       0.442| 0.632|   1|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|             4|   acoustic|          2011-11-21|\n",
      "|      10|4mzP5mHkRvGxdhdGd...|        Zack Tabudlo|             Episode|Give Me Your Forever|        74|     244800|   FALSE|       0.627| 0.363|   8|  -8.127|   1|     0.0291|       0.279|               0|  0.0928|  0.301| 99.905|             4|   acoustic|          2019-01-05|\n",
      "|      11|5ivF4eQBqJiVL5IAE...|          Jason Mraz|Love Is a Four Le...|     I Won't Give Up|        69|     240165|   FALSE|       0.483| 0.303|   4| -10.058|   1|     0.0429|       0.694|               0|   0.115|  0.139|133.406|             3|   acoustic|          2018-09-30|\n",
      "|      12|4ptDJbJl35d7gQfeN...|            Dan Berk|                Solo|                Solo|        52|     198712|   FALSE|       0.489| 0.314|   7|  -9.245|   0|     0.0331|       0.749|               0|   0.113|  0.607|124.234|             4|   acoustic|          2016-05-11|\n",
      "|      13|0X9MxHR1rTkEHDjp9...|       Anna Hamilton|            Bad Liar|            Bad Liar|        62|     248448|   FALSE|       0.691| 0.234|   3|  -6.441|   1|     0.0285|       0.777|               0|    0.12|  0.209| 87.103|             4|   acoustic|          2019-05-29|\n",
      "|      14|4LbWtBkN82ZRhz9jq...|Chord Overstreet;...|     Hold On (Remix)|     Hold On - Remix|        56|     188133|   FALSE|       0.755|  0.78|   2|  -6.084|   1|     0.0327|       0.124|        2.83E-05|   0.121|  0.387|120.004|             4|   acoustic|          2014-09-30|\n",
      "|      15|1KHdq8NK9QxnGjdXb...|         Landon Pigg|   The Boy Who Never|Falling in Love a...|        58|     244986|   FALSE|       0.489| 0.561|   4|  -7.933|   1|     0.0274|         0.2|        4.56E-05|   0.179|  0.238| 83.457|             3|   acoustic|          2017-06-20|\n",
      "|      16|6xKeQgzfjixSUld14...|Andrew Foy;Renee Foy|ily (i love you b...|ily (i love you b...|        56|     129750|   FALSE|       0.706| 0.112|   2| -18.098|   1|     0.0391|       0.827|        4.03E-06|   0.125|  0.414|110.154|             4|   acoustic|          2018-03-20|\n",
      "|      17|4Yo0igmcoNyat1sec...|Andrew Foy;Renee Foy|         At My Worst|         At My Worst|        54|     169728|   FALSE|       0.795|0.0841|  10|  -18.09|   0|     0.0461|       0.742|        1.17E-05|  0.0853|  0.609| 91.803|             4|   acoustic|          2013-10-23|\n",
      "|      18|2qLMf6TuEC3ruGJg4...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        68|     189613|   FALSE|       0.625| 0.414|   0|    -8.7|   1|     0.0369|       0.294|               0|   0.151|  0.669|130.088|             4|   acoustic|          2015-01-14|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+----+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the dataset\n",
    "music.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73288cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Column number with header name \n",
    "music=spark.read.option('header','true').csv(r'C:\\Users\\AJINKYA\\Desktop\\Final_Project-main\\final_spotify.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e9ae51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index_id: string, track_id: string, artists: string, album_name: string, track_name: string, popularity: string, duration_ms: string, explicit: string, danceability: string, energy: string, key: string, loudness: string, mode: string, speechiness: string, acousticness: string, instrumentalness: string, liveness: string, valence: string, tempo: string, time_signature: string, track_genre: string, spotify_release_date: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288b5e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "|index_id|            track_id|             artists|          album_name|          track_name|popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|time_signature|track_genre|spotify_release_date|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "|       0|5SuOikwiRyPMVoIQD...|         Gen Hoshino|              Comedy|              Comedy|        73|     230666|   FALSE|       0.676| 0.461|  1|  -6.746|   0|      0.143|      0.0322|        1.01E-06|   0.358|  0.715| 87.917|             4|   acoustic|          2016-12-31|\n",
      "|       1|4qPNDBW1i3p13qLCt...|        Ben Woodward|    Ghost (Acoustic)|    Ghost - Acoustic|        55|     149610|   FALSE|        0.42| 0.166|  1| -17.235|   1|     0.0763|       0.924|        5.56E-06|   0.101|  0.267| 77.489|             4|   acoustic|          2009-02-23|\n",
      "|       2|1iJBSr7s7jYXzM8EG...|Ingrid Michaelson...|      To Begin Again|      To Begin Again|        57|     210826|   FALSE|       0.438| 0.359|  0|  -9.734|   1|     0.0557|        0.21|               0|   0.117|   0.12| 76.332|             4|   acoustic|          2014-03-26|\n",
      "|       3|6lfxq3CG4xtTiEg7o...|        Kina Grannis|Crazy Rich Asians...|Can't Help Fallin...|        71|     201933|   FALSE|       0.266|0.0596|  0| -18.515|   1|     0.0363|       0.905|        7.07E-05|   0.132|  0.143| 181.74|             3|   acoustic|          2013-02-01|\n",
      "|       4|5vjLSffimiIP26QG5...|    Chord Overstreet|             Hold On|             Hold On|        82|     198853|   FALSE|       0.618| 0.443|  2|  -9.681|   1|     0.0526|       0.469|               0|  0.0829|  0.167|119.949|             4|   acoustic|          2011-12-14|\n",
      "|       5|01MVOl9KtVTNfFiBU...|        Tyrone Wells|Days I Will Remember|Days I Will Remember|        58|     214240|   FALSE|       0.688| 0.481|  6|  -8.807|   1|      0.105|       0.289|               0|   0.189|  0.666| 98.017|             4|   acoustic|          2013-08-31|\n",
      "|       6|6Vc5wAMmXdKIAM7WU...|A Great Big World...|Is There Anybody ...|       Say Something|        74|     229400|   FALSE|       0.407| 0.147|  2|  -8.822|   1|     0.0355|       0.857|        2.89E-06|  0.0913| 0.0765|141.284|             3|   acoustic|          2020-08-23|\n",
      "|       7|1EzrEOXmMH3G43AXT...|          Jason Mraz|We Sing. We Dance...|           I'm Yours|        80|     242946|   FALSE|       0.703| 0.444| 11|  -9.331|   1|     0.0417|       0.559|               0|  0.0973|  0.712| 150.96|             4|   acoustic|          2019-08-29|\n",
      "|       8|0IktbUcnAGrvD03AW...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        74|     189613|   FALSE|       0.625| 0.414|  0|    -8.7|   1|     0.0369|       0.294|               0|   0.151|  0.669|130.088|             4|   acoustic|          2018-03-18|\n",
      "|       9|7k9GuJYLp2AzqokyE...|      Ross Copperman|              Hunger|              Hunger|        56|     205594|   FALSE|       0.442| 0.632|  1|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|             4|   acoustic|          2011-11-21|\n",
      "|      10|4mzP5mHkRvGxdhdGd...|        Zack Tabudlo|             Episode|Give Me Your Forever|        74|     244800|   FALSE|       0.627| 0.363|  8|  -8.127|   1|     0.0291|       0.279|               0|  0.0928|  0.301| 99.905|             4|   acoustic|          2019-01-05|\n",
      "|      11|5ivF4eQBqJiVL5IAE...|          Jason Mraz|Love Is a Four Le...|     I Won't Give Up|        69|     240165|   FALSE|       0.483| 0.303|  4| -10.058|   1|     0.0429|       0.694|               0|   0.115|  0.139|133.406|             3|   acoustic|          2018-09-30|\n",
      "|      12|4ptDJbJl35d7gQfeN...|            Dan Berk|                Solo|                Solo|        52|     198712|   FALSE|       0.489| 0.314|  7|  -9.245|   0|     0.0331|       0.749|               0|   0.113|  0.607|124.234|             4|   acoustic|          2016-05-11|\n",
      "|      13|0X9MxHR1rTkEHDjp9...|       Anna Hamilton|            Bad Liar|            Bad Liar|        62|     248448|   FALSE|       0.691| 0.234|  3|  -6.441|   1|     0.0285|       0.777|               0|    0.12|  0.209| 87.103|             4|   acoustic|          2019-05-29|\n",
      "|      14|4LbWtBkN82ZRhz9jq...|Chord Overstreet;...|     Hold On (Remix)|     Hold On - Remix|        56|     188133|   FALSE|       0.755|  0.78|  2|  -6.084|   1|     0.0327|       0.124|        2.83E-05|   0.121|  0.387|120.004|             4|   acoustic|          2014-09-30|\n",
      "|      15|1KHdq8NK9QxnGjdXb...|         Landon Pigg|   The Boy Who Never|Falling in Love a...|        58|     244986|   FALSE|       0.489| 0.561|  4|  -7.933|   1|     0.0274|         0.2|        4.56E-05|   0.179|  0.238| 83.457|             3|   acoustic|          2017-06-20|\n",
      "|      16|6xKeQgzfjixSUld14...|Andrew Foy;Renee Foy|ily (i love you b...|ily (i love you b...|        56|     129750|   FALSE|       0.706| 0.112|  2| -18.098|   1|     0.0391|       0.827|        4.03E-06|   0.125|  0.414|110.154|             4|   acoustic|          2018-03-20|\n",
      "|      17|4Yo0igmcoNyat1sec...|Andrew Foy;Renee Foy|         At My Worst|         At My Worst|        54|     169728|   FALSE|       0.795|0.0841| 10|  -18.09|   0|     0.0461|       0.742|        1.17E-05|  0.0853|  0.609| 91.803|             4|   acoustic|          2013-10-23|\n",
      "|      18|2qLMf6TuEC3ruGJg4...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        68|     189613|   FALSE|       0.625| 0.414|  0|    -8.7|   1|     0.0369|       0.294|               0|   0.151|  0.669|130.088|             4|   acoustic|          2015-01-14|\n",
      "|      19|6CgNoAbFJ4Q4Id4Ej...|Boyce Avenue;Bea ...|Cover Sessions, V...|          Photograph|        67|     260186|   FALSE|       0.717|  0.32|  3|  -8.393|   1|     0.0283|        0.83|               0|   0.107|  0.322|107.946|             4|   acoustic|          2017-11-30|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying the edited file\n",
    "music.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5a7fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index_id: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- duration_ms: string (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: string (nullable = true)\n",
      " |-- energy: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- loudness: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- speechiness: string (nullable = true)\n",
      " |-- acousticness: string (nullable = true)\n",
      " |-- instrumentalness: string (nullable = true)\n",
      " |-- liveness: string (nullable = true)\n",
      " |-- valence: string (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      " |-- spotify_release_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the schema of the dataset\n",
    "music.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35259ba",
   "metadata": {},
   "source": [
    "TYPECASTING to use data in valid datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24b6fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the datatype of the columns\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "music = music.withColumn(\"popularity\",music[\"popularity\"].cast(IntegerType()))\n",
    "music = music.withColumn(\"duration_ms\",music[\"duration_ms\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"explicit\",music[\"explicit\"].cast(BooleanType()))\n",
    "music = music.withColumn(\"danceability\",music[\"danceability\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"energy\",music[\"energy\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"key\",music[\"key\"].cast(IntegerType()))\n",
    "music = music.withColumn(\"loudness\",music[\"loudness\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"mode\",music[\"mode\"].cast(BooleanType()))\n",
    "music = music.withColumn(\"speechiness\",music[\"speechiness\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"acousticness\",music[\"acousticness\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"instrumentalness\",music[\"instrumentalness\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"liveness\",music[\"liveness\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"valence\",music[\"valence\"].cast(DoubleType()))\n",
    "music = music.withColumn(\"spotify_release_date\",music[\"spotify_release_date\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9cf04be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index_id: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: boolean (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      " |-- spotify_release_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to verify whether the data types are changed or not\n",
    "music.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7816a6c",
   "metadata": {},
   "source": [
    "REMOVING THE NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a97bee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113865"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting null values\n",
    "music.na.drop(how='any').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824767e",
   "metadata": {},
   "source": [
    "Total 135 rows with any null value are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99de511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+-----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "|index_id|            track_id|             artists|          album_name|          track_name|popularity|duration_ms|explicit|danceability|energy|key|loudness| mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|time_signature|track_genre|spotify_release_date|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+-----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "|       0|5SuOikwiRyPMVoIQD...|         Gen Hoshino|              Comedy|              Comedy|        73|   230666.0|   false|       0.676| 0.461|  1|  -6.746|false|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|             4|   acoustic|          2016-12-31|\n",
      "|       1|4qPNDBW1i3p13qLCt...|        Ben Woodward|    Ghost (Acoustic)|    Ghost - Acoustic|        55|   149610.0|   false|        0.42| 0.166|  1| -17.235| true|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|             4|   acoustic|          2009-02-23|\n",
      "|       2|1iJBSr7s7jYXzM8EG...|Ingrid Michaelson...|      To Begin Again|      To Begin Again|        57|   210826.0|   false|       0.438| 0.359|  0|  -9.734| true|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|             4|   acoustic|          2014-03-26|\n",
      "|       3|6lfxq3CG4xtTiEg7o...|        Kina Grannis|Crazy Rich Asians...|Can't Help Fallin...|        71|   201933.0|   false|       0.266|0.0596|  0| -18.515| true|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|             3|   acoustic|          2013-02-01|\n",
      "|       4|5vjLSffimiIP26QG5...|    Chord Overstreet|             Hold On|             Hold On|        82|   198853.0|   false|       0.618| 0.443|  2|  -9.681| true|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949|             4|   acoustic|          2011-12-14|\n",
      "|       5|01MVOl9KtVTNfFiBU...|        Tyrone Wells|Days I Will Remember|Days I Will Remember|        58|   214240.0|   false|       0.688| 0.481|  6|  -8.807| true|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017|             4|   acoustic|          2013-08-31|\n",
      "|       6|6Vc5wAMmXdKIAM7WU...|A Great Big World...|Is There Anybody ...|       Say Something|        74|   229400.0|   false|       0.407| 0.147|  2|  -8.822| true|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|             3|   acoustic|          2020-08-23|\n",
      "|       7|1EzrEOXmMH3G43AXT...|          Jason Mraz|We Sing. We Dance...|           I'm Yours|        80|   242946.0|   false|       0.703| 0.444| 11|  -9.331| true|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|             4|   acoustic|          2019-08-29|\n",
      "|       8|0IktbUcnAGrvD03AW...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        74|   189613.0|   false|       0.625| 0.414|  0|    -8.7| true|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|             4|   acoustic|          2018-03-18|\n",
      "|       9|7k9GuJYLp2AzqokyE...|      Ross Copperman|              Hunger|              Hunger|        56|   205594.0|   false|       0.442| 0.632|  1|   -6.77| true|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|             4|   acoustic|          2011-11-21|\n",
      "|      10|4mzP5mHkRvGxdhdGd...|        Zack Tabudlo|             Episode|Give Me Your Forever|        74|   244800.0|   false|       0.627| 0.363|  8|  -8.127| true|     0.0291|       0.279|             0.0|  0.0928|  0.301| 99.905|             4|   acoustic|          2019-01-05|\n",
      "|      11|5ivF4eQBqJiVL5IAE...|          Jason Mraz|Love Is a Four Le...|     I Won't Give Up|        69|   240165.0|   false|       0.483| 0.303|  4| -10.058| true|     0.0429|       0.694|             0.0|   0.115|  0.139|133.406|             3|   acoustic|          2018-09-30|\n",
      "|      12|4ptDJbJl35d7gQfeN...|            Dan Berk|                Solo|                Solo|        52|   198712.0|   false|       0.489| 0.314|  7|  -9.245|false|     0.0331|       0.749|             0.0|   0.113|  0.607|124.234|             4|   acoustic|          2016-05-11|\n",
      "|      13|0X9MxHR1rTkEHDjp9...|       Anna Hamilton|            Bad Liar|            Bad Liar|        62|   248448.0|   false|       0.691| 0.234|  3|  -6.441| true|     0.0285|       0.777|             0.0|    0.12|  0.209| 87.103|             4|   acoustic|          2019-05-29|\n",
      "|      14|4LbWtBkN82ZRhz9jq...|Chord Overstreet;...|     Hold On (Remix)|     Hold On - Remix|        56|   188133.0|   false|       0.755|  0.78|  2|  -6.084| true|     0.0327|       0.124|         2.83E-5|   0.121|  0.387|120.004|             4|   acoustic|          2014-09-30|\n",
      "|      15|1KHdq8NK9QxnGjdXb...|         Landon Pigg|   The Boy Who Never|Falling in Love a...|        58|   244986.0|   false|       0.489| 0.561|  4|  -7.933| true|     0.0274|         0.2|         4.56E-5|   0.179|  0.238| 83.457|             3|   acoustic|          2017-06-20|\n",
      "|      16|6xKeQgzfjixSUld14...|Andrew Foy;Renee Foy|ily (i love you b...|ily (i love you b...|        56|   129750.0|   false|       0.706| 0.112|  2| -18.098| true|     0.0391|       0.827|         4.03E-6|   0.125|  0.414|110.154|             4|   acoustic|          2018-03-20|\n",
      "|      17|4Yo0igmcoNyat1sec...|Andrew Foy;Renee Foy|         At My Worst|         At My Worst|        54|   169728.0|   false|       0.795|0.0841| 10|  -18.09|false|     0.0461|       0.742|         1.17E-5|  0.0853|  0.609| 91.803|             4|   acoustic|          2013-10-23|\n",
      "|      18|2qLMf6TuEC3ruGJg4...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        68|   189613.0|   false|       0.625| 0.414|  0|    -8.7| true|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|             4|   acoustic|          2015-01-14|\n",
      "|      19|6CgNoAbFJ4Q4Id4Ej...|Boyce Avenue;Bea ...|Cover Sessions, V...|          Photograph|        67|   260186.0|   false|       0.717|  0.32|  3|  -8.393| true|     0.0283|        0.83|             0.0|   0.107|  0.322|107.946|             4|   acoustic|          2017-11-30|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+-----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f38a3d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index_id: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: boolean (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      " |-- spotify_release_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62abd937",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9f171e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "X = music.drop(\"track_id\", \"index_id\", \"artists\", \"album_name\", \"track_name\", \"track_genre\", \"duration_ms\", \"explicit\", \"key\",\"mode\", \"spotify_release_date\")\n",
    "\n",
    "X = X.select([col(c).cast(\"double\") for c in X.columns])  # convert columns to double type\n",
    "\n",
    "\n",
    "y = music.select(\"track_genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f56d5611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d65542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector \n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dbe70f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['popularity',\n",
       " 'danceability',\n",
       " 'energy',\n",
       " 'loudness',\n",
       " 'speechiness',\n",
       " 'acousticness',\n",
       " 'instrumentalness',\n",
       " 'liveness',\n",
       " 'valence',\n",
       " 'tempo',\n",
       " 'time_signature']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15c8e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['popularity','danceability','energy','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "46b556f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "53be0b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+--------+-----------+------------+----------------+--------+-------+-------+--------------+------------------------------------------------------------------------+\n",
      "|popularity|danceability|energy|loudness|speechiness|acousticness|instrumentalness|liveness|valence|tempo  |time_signature|features                                                                |\n",
      "+----------+------------+------+--------+-----------+------------+----------------+--------+-------+-------+--------------+------------------------------------------------------------------------+\n",
      "|73.0      |0.676       |0.461 |-6.746  |0.143      |0.0322      |1.01E-6         |0.358   |0.715  |87.917 |4.0           |[73.0,0.676,0.461,-6.746,0.143,0.0322,1.01E-6,0.358,0.715,87.917,4.0]   |\n",
      "|55.0      |0.42        |0.166 |-17.235 |0.0763     |0.924       |5.56E-6         |0.101   |0.267  |77.489 |4.0           |[55.0,0.42,0.166,-17.235,0.0763,0.924,5.56E-6,0.101,0.267,77.489,4.0]   |\n",
      "|57.0      |0.438       |0.359 |-9.734  |0.0557     |0.21        |0.0             |0.117   |0.12   |76.332 |4.0           |[57.0,0.438,0.359,-9.734,0.0557,0.21,0.0,0.117,0.12,76.332,4.0]         |\n",
      "|71.0      |0.266       |0.0596|-18.515 |0.0363     |0.905       |7.07E-5         |0.132   |0.143  |181.74 |3.0           |[71.0,0.266,0.0596,-18.515,0.0363,0.905,7.07E-5,0.132,0.143,181.74,3.0] |\n",
      "|82.0      |0.618       |0.443 |-9.681  |0.0526     |0.469       |0.0             |0.0829  |0.167  |119.949|4.0           |[82.0,0.618,0.443,-9.681,0.0526,0.469,0.0,0.0829,0.167,119.949,4.0]     |\n",
      "|58.0      |0.688       |0.481 |-8.807  |0.105      |0.289       |0.0             |0.189   |0.666  |98.017 |4.0           |[58.0,0.688,0.481,-8.807,0.105,0.289,0.0,0.189,0.666,98.017,4.0]        |\n",
      "|74.0      |0.407       |0.147 |-8.822  |0.0355     |0.857       |2.89E-6         |0.0913  |0.0765 |141.284|3.0           |[74.0,0.407,0.147,-8.822,0.0355,0.857,2.89E-6,0.0913,0.0765,141.284,3.0]|\n",
      "|80.0      |0.703       |0.444 |-9.331  |0.0417     |0.559       |0.0             |0.0973  |0.712  |150.96 |4.0           |[80.0,0.703,0.444,-9.331,0.0417,0.559,0.0,0.0973,0.712,150.96,4.0]      |\n",
      "|74.0      |0.625       |0.414 |-8.7    |0.0369     |0.294       |0.0             |0.151   |0.669  |130.088|4.0           |[74.0,0.625,0.414,-8.7,0.0369,0.294,0.0,0.151,0.669,130.088,4.0]        |\n",
      "|56.0      |0.442       |0.632 |-6.77   |0.0295     |0.426       |0.00419         |0.0735  |0.196  |78.899 |4.0           |[56.0,0.442,0.632,-6.77,0.0295,0.426,0.00419,0.0735,0.196,78.899,4.0]   |\n",
      "|74.0      |0.627       |0.363 |-8.127  |0.0291     |0.279       |0.0             |0.0928  |0.301  |99.905 |4.0           |[74.0,0.627,0.363,-8.127,0.0291,0.279,0.0,0.0928,0.301,99.905,4.0]      |\n",
      "|69.0      |0.483       |0.303 |-10.058 |0.0429     |0.694       |0.0             |0.115   |0.139  |133.406|3.0           |[69.0,0.483,0.303,-10.058,0.0429,0.694,0.0,0.115,0.139,133.406,3.0]     |\n",
      "|52.0      |0.489       |0.314 |-9.245  |0.0331     |0.749       |0.0             |0.113   |0.607  |124.234|4.0           |[52.0,0.489,0.314,-9.245,0.0331,0.749,0.0,0.113,0.607,124.234,4.0]      |\n",
      "|62.0      |0.691       |0.234 |-6.441  |0.0285     |0.777       |0.0             |0.12    |0.209  |87.103 |4.0           |[62.0,0.691,0.234,-6.441,0.0285,0.777,0.0,0.12,0.209,87.103,4.0]        |\n",
      "|56.0      |0.755       |0.78  |-6.084  |0.0327     |0.124       |2.83E-5         |0.121   |0.387  |120.004|4.0           |[56.0,0.755,0.78,-6.084,0.0327,0.124,2.83E-5,0.121,0.387,120.004,4.0]   |\n",
      "|58.0      |0.489       |0.561 |-7.933  |0.0274     |0.2         |4.56E-5         |0.179   |0.238  |83.457 |3.0           |[58.0,0.489,0.561,-7.933,0.0274,0.2,4.56E-5,0.179,0.238,83.457,3.0]     |\n",
      "|56.0      |0.706       |0.112 |-18.098 |0.0391     |0.827       |4.03E-6         |0.125   |0.414  |110.154|4.0           |[56.0,0.706,0.112,-18.098,0.0391,0.827,4.03E-6,0.125,0.414,110.154,4.0] |\n",
      "|54.0      |0.795       |0.0841|-18.09  |0.0461     |0.742       |1.17E-5         |0.0853  |0.609  |91.803 |4.0           |[54.0,0.795,0.0841,-18.09,0.0461,0.742,1.17E-5,0.0853,0.609,91.803,4.0] |\n",
      "|68.0      |0.625       |0.414 |-8.7    |0.0369     |0.294       |0.0             |0.151   |0.669  |130.088|4.0           |[68.0,0.625,0.414,-8.7,0.0369,0.294,0.0,0.151,0.669,130.088,4.0]        |\n",
      "|67.0      |0.717       |0.32  |-8.393  |0.0283     |0.83        |0.0             |0.107   |0.322  |107.946|4.0           |[67.0,0.717,0.32,-8.393,0.0283,0.83,0.0,0.107,0.322,107.946,4.0]        |\n",
      "+----------+------------+------+--------+-----------+------------+----------------+--------+-------+-------+--------------+------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4cfc60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "182e4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol = \"features\", outputCol = \"scaledfeatures\", withStd = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b04f5d46",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1042.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 75) (SeedingYounger executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3557/135181436: (struct<popularity:double,danceability:double,energy:double,loudness:double,speechiness:double,acousticness:double,instrumentalness:double,liveness:double,valence:double,tempo:double,time_signature:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3557/135181436: (struct<popularity:double,danceability:double,energy:double,loudness:double,speechiness:double,acousticness:double,instrumentalness:double,liveness:double,valence:double,tempo:double,time_signature:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4752\\452907699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscalerModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1042.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 75) (SeedingYounger executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3557/135181436: (struct<popularity:double,danceability:double,energy:double,loudness:double,speechiness:double,acousticness:double,instrumentalness:double,liveness:double,valence:double,tempo:double,time_signature:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3557/135181436: (struct<popularity:double,danceability:double,energy:double,loudness:double,speechiness:double,acousticness:double,instrumentalness:double,liveness:double,valence:double,tempo:double,time_signature:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "scalerModel = scaler.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad184f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
